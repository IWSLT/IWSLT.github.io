### Pretrained models

The follow pre-trained language models are considered parts of the training data and freely usable to build the SLT systems:

  * [Wav2vec 2.0](https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/README.md)
  * [Hubert](https://github.com/pytorch/fairseq/tree/main/examples/hubert)
  * [WavLM](https://github.com/microsoft/unilm/tree/master/wavlm)
  * [SpeechLM](https://github.com/microsoft/unilm/tree/master/speechlm)
  * [data2vec](https://github.com/facebookresearch/fairseq/tree/main/examples/data2vec)
  * [MBART](https://github.com/pytorch/fairseq/blob/main/examples/mbart/README.md)
  * [MBART50](https://github.com/pytorch/fairseq/tree/main/examples/multilingual#mbart50-models)
  * [M2M100](https://github.com/pytorch/fairseq/tree/main/examples/m2m_100)
  * [Delta LM](https://github.com/microsoft/unilm/tree/master/deltalm)
  * [T5](https://github.com/google-research/text-to-text-transfer-transformer)
  * [BLOOM(only the small 560m paramter version)](https://huggingface.co/bigscience/bloom-560m#model-details)
