---
permalink: /lectures/
title: "Lectures"
classes: wide
---

<br/>
The ISCA SIGSLT lecture series are held periodically on Zoom, with recordings linked here after the talk for those who cannot join live.
Zoom links are posted to the [SIGSLT google group](https://groups.google.com/g/sigslt){:target="_blank"}.


## Isochrony for Automatic Dubbing
Prashant Mathur, Senior Applied Scientist in Amazon AI.

**Date:**  
<i class="fas fa-calendar-day"></i> Thurs 5 May 2022, 23:00 UTC [(8am Fri JST / 1am Fri CET / 7pm Thurs EST / 4pm Thurs PST)](https://www.timeanddate.com/worldclock/converter.html?iso=20220505T230000&p1=1440&p2=248&p3=5805&p4=224&p5=179)

**Recording:**  
<i class="fas fa-video"></i> Watch on [YouTube](https://youtu.be/PaxRvWzBlOI){:target="_blank"}  

**Abstract:**  
In this talk, I will focus on the problem of isochrony in automatic dubbing for media localization where the goal is to maintain synchrony of translated dialogues and original video when the speakers are on-screen. This problem requires that we first identify the location of pauses in the translation and then ensure the speech-pause arrangement as in the source audio. I will first introduce our approaches on aligning the pauses from source speech to target translation (prosodic alignment), then review a recent work on isometric machine translation and a follow-up work on isochrony aware MT. I will end the talk by providing a brief overview of the [Isometric SLT shared task](https://iwslt.org/2022/isometric) we are organizing as a part of IWSLT.

**Bio:**  
Prashant is a Senior Applied Scientist in Amazon AI. His research focuses on improving the state of the art for machine translation, and domain adaptation with special attention to the problem of Automatic Dubbing. Prior to Amazon, he was a research scientist at eBay working on language generation with structured data and machine translation for eBay’s catalog content.


## Towards Augmented Speech Translation: Joint Speech Translation and Named Entity Recognition
Marco Gaido, Ph.D. student at Fondazione Bruno Kessler (FBK), Italy

**Date:**  
<i class="fas fa-calendar-day"></i> Wed 6 April 2022, 16:00 UTC [(1am JST / 6pm CET / 12pm EST / 9am PST)](https://www.timeanddate.com/worldclock/converter.html?iso=20220406T160000&p1=1440&p2=248&p3=5805&p4=419&p5=224)

**Recording:**  
<i class="fas fa-video"></i> Watch on [YouTube](https://www.youtube.com/watch?v=g2fIcjVWgXY){:target="_blank"}  

**Abstract:**  
Translation is a complex task involving different levels of understanding of the content being handled. The process involves grasping the semantic meaning of the source, which is conveyed through the mentioned (named) entities, the specific terminology indicating peculiar elements, and the relationships between them. However, these aspects remain implicit in the output of automatic translation systems exclusively designed to generate fluent and adequate text. Going beyond these standard output quality objectives, we envisage "augmented translation" as the task of jointly generating the output together with explicit meaning-related enrichments suitable for downstream use. In this talk, we will discuss the application of this idea to the domain of live interpreting, in which handling named entities represents a daunting task. In this framework, we will present our work on systems that jointly translate speech and recognize named entities, discussing the main challenges, possible approaches, data requirements, experimental results, and future research directions.


## Simultaneous Speech-to-Speech Translation with Transformer-based Incremental ASR, MT, and TTS
Katsuhito Sudoh Ph.D., Associate Professor at the Nara Institute of Science and Technology (NAIST), Japan

**Date:**  
<i class="fas fa-calendar-day"></i> Tue 1 March 2022, 8:00 UTC [(5pm JST / 9am CET / 12am PCT)](https://www.timeanddate.com/worldclock/converter.html?iso=20220301T080000&p1=1440&p2=248&p3=5805&p4=224)

**Recording:**  
<i class="fas fa-video"></i> Watch on [YouTube](https://www.youtube.com/watch?v=PH6Zlki-1pg&list=PLMS8PIkS6c4jU-4KZdhFErTYMP7KmFRw-){:target="_blank"}  

**Abstract:**  
I’ll talk about our English-to-Japanese simultaneous speech-to-speech translation (S2ST) system. It has three Transformer-based incremental processing modules for S2ST: automatic speech recognition (ASR), machine translation (MT), and text-to-speech synthesis (TTS). We also evaluated its system-level latency in addition to the module-level latency and accuracy.  
([This work](https://ieeexplore.ieee.org/document/9660477) was originally presented at Oriental COCOSDA 2021). 

