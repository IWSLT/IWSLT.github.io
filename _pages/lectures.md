---
permalink: /lectures/
title: "Lectures"
classes: wide
---

<br/>
The ISCA SIGSLT lecture series are held periodically on Zoom, with recordings linked here after the talk for those who cannot join live.
Zoom links are posted to the [SIGSLT google group](https://groups.google.com/g/sigslt){:target="_blank"}.


## Towards Augmented Speech Translation: Joint Speech Translation and Named Entity Recognition
Marco Gaido, Ph.D. student at Fondazione Bruno Kessler (FBK), Italy

**Date:**  
<i class="fas fa-calendar-day"></i> Wed 6 April 2022, 16:00 UTC [(1am JST / 6pm CET / 12pm EST / 9am PST)](https://www.timeanddate.com/worldclock/converter.html?iso=20220406T160000&p1=1440&p2=248&p3=5805&p4=419&p5=224)

**Recording:**  
<i class="fas fa-video"></i> Watch on [YouTube](https://www.youtube.com/watch?v=g2fIcjVWgXY){:target="_blank"}  

**Abstract:**  
Translation is a complex task involving different levels of understanding of the content being handled. The process involves grasping the semantic meaning of the source, which is conveyed through the mentioned (named) entities, the specific terminology indicating peculiar elements, and the relationships between them. However, these aspects remain implicit in the output of automatic translation systems exclusively designed to generate fluent and adequate text. Going beyond these standard output quality objectives, we envisage "augmented translation" as the task of jointly generating the output together with explicit meaning-related enrichments suitable for downstream use. In this talk, we will discuss the application of this idea to the domain of live interpreting, in which handling named entities represents a daunting task. In this framework, we will present our work on systems that jointly translate speech and recognize named entities, discussing the main challenges, possible approaches, data requirements, experimental results, and future research directions.


## Simultaneous Speech-to-Speech Translation with Transformer-based Incremental ASR, MT, and TTS
Katsuhito Sudoh Ph.D., Associate Professor at the Nara Institute of Science and Technology (NAIST), Japan

**Date:**  
<i class="fas fa-calendar-day"></i> Tue 1 March 2022, 8:00 UTC [(5pm JST / 9am CET / 12am PCT)](https://www.timeanddate.com/worldclock/converter.html?iso=20220301T080000&p1=1440&p2=248&p3=5805&p4=224)

**Recording:**  
<i class="fas fa-video"></i> Watch on [YouTube](https://www.youtube.com/watch?v=PH6Zlki-1pg&list=PLMS8PIkS6c4jU-4KZdhFErTYMP7KmFRw-){:target="_blank"}  

**Abstract:**  
Iâ€™ll talk about our English-to-Japanese simultaneous speech-to-speech translation (S2ST) system. It has three Transformer-based incremental processing modules for S2ST: automatic speech recognition (ASR), machine translation (MT), and text-to-speech synthesis (TTS). We also evaluated its system-level latency in addition to the module-level latency and accuracy.  
([This work](https://ieeexplore.ieee.org/document/9660477) was originally presented at Oriental COCOSDA 2021). 

